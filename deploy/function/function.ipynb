{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb282c5-2ca6-4a80-98de-20361fee0b3d",
   "metadata": {},
   "source": [
    "# Notebook with API code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "43f73b28-d670-4639-a7f4-c4c67c13d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unidecode\n",
    "from datetime import timedelta\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import os\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0027fc8e-451e-46b3-8f21-776a3897858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition\n",
    "def pretty_string(column):\n",
    "    column_space = ' '.join(column.split())\n",
    "    return unidecode.unidecode(column_space.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1d524-ea90-441c-a888-8b1f51fc351f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating model / update model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba3f7b61-d07c-442b-905e-28f8f409093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(PATH):\n",
    "\n",
    "    orders_PATH = PATH + \"/olist_orders_dataset.csv\"\n",
    "    order_items_PATH = PATH + \"/olist_order_items_dataset.csv\"\n",
    "    order_payments_PATH = PATH + \"/olist_order_payment_dataset.csv\"\n",
    "    order_reviews_PATH = PATH + \"/olist_order_reviews_dataset.csv\"\n",
    "    customers_PATH = PATH + \"/olist_customers_dataset.csv\"\n",
    "    cities_PATH = PATH + \"/BRAZIL_CITIES.csv\"\n",
    "\n",
    "    # Load dataset from Olist Db\n",
    "    #orders = pd.read_csv(orders_PATH)\n",
    "    #order_items = pd.read_csv(order_items_PATH)\n",
    "    #order_payments = pd.read_csv(order_payments_PATH)\n",
    "    #order_reviews = pd.read_csv(order_reviews_PATH)\n",
    "    #customers = pd.read_csv(customers_PATH)\n",
    "    #cities = pd.read_csv(cities_PATH, sep=';')\n",
    "\n",
    "    orders = pd.read_csv('olist_orders_dataset.csv')\n",
    "    order_items = pd.read_csv('olist_order_items_dataset.csv')\n",
    "    order_payments = pd.read_csv('olist_order_payments_dataset.csv')\n",
    "    order_reviews = pd.read_csv('olist_order_reviews_dataset.csv')\n",
    "    customers = pd.read_csv('olist_customers_dataset.csv')\n",
    "    cities = pd.read_csv('BRAZIL_CITIES.csv', sep=';')\n",
    "    \n",
    "    #################\n",
    "    ## Orders dataset\n",
    "    #################\n",
    "\n",
    "    # Keep only delivered orders and variable of interest\n",
    "    orders = orders.loc[orders['order_status'] \\\n",
    "                        == 'delivered', ['order_id',\n",
    "                                         'customer_id',\n",
    "                                         'order_purchase_timestamp']]\n",
    "\n",
    "    # Drop na In case of missing purchase timestamp\n",
    "    orders.dropna(axis=0, inplace=True)\n",
    "\n",
    "    #################\n",
    "    ## Order_items dataset\n",
    "    #################\n",
    "\n",
    "    # Groupe by items to calculated total price, total freight values\n",
    "    order_infos = order_items.groupby('order_id')\\\n",
    "        .agg(tot_price=('price', 'sum'),\n",
    "             tot_freight_value=('freight_value', 'sum')).reset_index()\n",
    "\n",
    "\n",
    "    #################\n",
    "    ## Order_reviews dataset\n",
    "    #################\n",
    "\n",
    "    # Keep only variable of interest\n",
    "    order_reviews = order_reviews.loc[:, ['review_id',\n",
    "                                          'order_id',\n",
    "                                          'review_score']]\n",
    "\n",
    "    #################\n",
    "    ## City size feature creation\n",
    "    #################\n",
    "\n",
    "    # Customer city case homogeneisation\n",
    "    customers['customer_city'] = customers['customer_city']\\\n",
    "        .apply(pretty_string)\n",
    "\n",
    "    # Keep variable of interest\n",
    "    cities = cities.loc[:, ['CITY',\n",
    "                            'STATE',\n",
    "                            'IBGE_RES_POP']]\n",
    "\n",
    "    # Case modification\n",
    "    cities['CITY'] = cities['CITY'].apply(pretty_string)\n",
    "\n",
    "    #################\n",
    "    ## Merging\n",
    "    #################\n",
    "\n",
    "    df_orders = orders.merge(order_infos,\n",
    "                             how='inner',\n",
    "                             on='order_id')\n",
    "\n",
    "\n",
    "    df_orders = df_orders.merge(order_reviews,\n",
    "                                how='inner',\n",
    "                                on='order_id')\n",
    "    df_orders.drop('review_id', axis=1, inplace=True)\n",
    "\n",
    "    df_orders = df_orders.merge(customers,\n",
    "                                how='inner',\n",
    "                                on='customer_id')\n",
    "    df_orders.drop('customer_id', axis=1, inplace=True)\n",
    "\n",
    "    df_orders = df_orders.merge(cities,\n",
    "                                how='inner',\n",
    "                                left_on=['customer_city',\n",
    "                                         'customer_state'],\n",
    "                                right_on=['CITY', 'STATE'])\n",
    "    col_drop = ['customer_zip_code_prefix',\n",
    "                'customer_state',\n",
    "                'customer_city',\n",
    "                'CITY',\n",
    "                'STATE']\n",
    "    df_orders.drop(col_drop, axis=1, inplace=True)\n",
    "\n",
    "    # Drop na In case of missing city population\n",
    "    df_orders.dropna(axis=0, inplace=True)\n",
    "\n",
    "    df_orders['order_purchase_timestamp'] = \\\n",
    "        pd.to_datetime(df_orders['order_purchase_timestamp'])\n",
    "    \n",
    "    #################\n",
    "    ## Automated outliers drop with isolation forrest\n",
    "    #################\n",
    "\n",
    "    # Creating features dataframe\n",
    "    X = df_orders.loc[:, ['tot_price',\n",
    "                          'tot_freight_value']]\n",
    "\n",
    "\n",
    "    # Scaling features using Robust Scaler\n",
    "    scaler = RobustScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Model fitting with reserach of O.O1 percent of outlier\n",
    "    model = IsolationForest(contamination=0.00001)\n",
    "\n",
    "    model.fit(X_scaled)\n",
    "\n",
    "    # Drop detected outliers\n",
    "\n",
    "    outliers = model.predict(X_scaled) == -1\n",
    "\n",
    "    df_orders.drop(df_orders[outliers].index,\n",
    "                   axis=0,\n",
    "                   inplace=True)\n",
    "    \n",
    "    #################\n",
    "    ## Features Creation\n",
    "    #################\n",
    "\n",
    "    # Snapshot of the last order timestamp\n",
    "    snapshot_date = df_orders['order_purchase_timestamp'].max() + timedelta(days=1)\n",
    "\n",
    "    # Clustering model features caculating\n",
    "    df_client = df_orders.groupby(['customer_unique_id']).apply(lambda x: pd.Series({\n",
    "        'recency': (snapshot_date - x['order_purchase_timestamp'].max()).days,\n",
    "        'frequency': x['order_id'].count(),\n",
    "        'monetary_value': (x['tot_price'] + x['tot_freight_value']).sum(),\n",
    "        'avg_review_score': x['review_score'].mean(),\n",
    "        'customer_city_size': x['IBGE_RES_POP'].mean()\n",
    "    }))\n",
    "\n",
    "\n",
    "    # We only keep client with at least 2 purchases\n",
    "    df_client = df_client[df_client['frequency'] > 1]\n",
    "    \n",
    "    #df_client.to_csv('base_dataset_model.csv')\n",
    "        \n",
    "    #################\n",
    "    ## Model fitting\n",
    "    #################\n",
    "\n",
    "    # Based model preprossessing\n",
    "    scaler = RobustScaler()\n",
    "\n",
    "    X_scaled = scaler.fit_transform(df_client)\n",
    "\n",
    "    # Five clusters model fitting\n",
    "    \n",
    "    best_model = KMeans(n_clusters=5,\n",
    "                        max_iter=1000,\n",
    "                        n_init=10,\n",
    "                        random_state=0)\n",
    "\n",
    "    best_model.fit(X_scaled)\n",
    "    \n",
    "    #################\n",
    "    ## Save model as pickle file\n",
    "    #################\n",
    "    import pickle\n",
    "\n",
    "    with open('pickle_model', 'wb') as file:\n",
    "        pickle.dump(best_model, file)\n",
    "\n",
    "    with open('pickle_scaler', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    \n",
    "    \n",
    "    return(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6a735d9-8a95-49f7-ad53-ae2d8ba755cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(max_iter=1000, n_clusters=5, random_state=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_model(PATH=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77069796-35f3-464b-bc95-e83116a7cff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # Open the saved model\n",
    "with open('pickle_model', 'rb') as file:\n",
    "    best_model = pickle.load(file)\n",
    "\n",
    "with open('pickle_scaler', 'rb') as file:\n",
    "    scaler = pickle.load(file)\n",
    "\n",
    "df = scaler.transform(customer_info)\n",
    "best_model.predict(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8a7f0e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02317497,  0.2       , -0.04763175, -0.125     ,  1.67201465]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class Olist_cluster:\n",
    "    '''Create a object to implement the olist clustering.\n",
    "    '''\n",
    "    def __init__(self, scaler_path:str, model_path:str):\n",
    "        self.scaler = self.get_scaler(scaler_path)\n",
    "        self.model = self.get_model(model_path)\n",
    "        self.olist_cluster = {\n",
    "            0: 'Former average',\n",
    "            1: 'Current average',\n",
    "            2: 'Big city average',\n",
    "            3: 'Unsatisfied',\n",
    "            4: 'Champions'}\n",
    "    \n",
    "    def get_model(self, model_path:str) -> KMeans:\n",
    "        '''Open the pkl file which store the model.\n",
    "        Arguments: \n",
    "            model_path: Path model with pkl extension\n",
    "        \n",
    "        Returns:\n",
    "            model: Model object\n",
    "        '''\n",
    "\n",
    "        with open(model_path,\"rb\") as f:\n",
    "            model = pickle.load(f)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_scaler(self, scaler_path:str) -> StandardScaler:\n",
    "        '''Open the pkl file which store the scaler.\n",
    "        Arguments: \n",
    "            scaler_path: Path scaler with pkl extension\n",
    "        \n",
    "        Returns:\n",
    "            scaler: scaler object\n",
    "        '''\n",
    "\n",
    "        with open(scaler_path,\"rb\") as f:\n",
    "            scaler = pickle.load(f)\n",
    "        \n",
    "        return scaler\n",
    "\n",
    "    def make_prediction(self, features:dict)->str:\n",
    "        '''Predicts the cluster.\n",
    "        Argument:\n",
    "            features: list\n",
    "        \n",
    "        return:\n",
    "            cluster: str\n",
    "        '''\n",
    "        features = np.array(list(features.values()))\n",
    "        features_scaled = self.scaler.transform(features.reshape(1,-1))\n",
    "        pred = self.model.predict(features_scaled.reshape(1,-1))[0]\n",
    "        cluster_pred = self.olist_cluster[pred]\n",
    "        return cluster_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409353c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe71d806a2dab62c79a3b947b7807b1541227d8a1cc8e187d32346ae863d9d7d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
